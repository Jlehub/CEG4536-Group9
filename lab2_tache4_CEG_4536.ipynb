{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratoire 2 de CEG 4536\n"
      ],
      "metadata": {
        "id": "gkbs1NnAR9vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXkJtOEl7M7-",
        "outputId": "48fe41fb-f574-423f-f48e-28beffb76c30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  5 02:29:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5uW4f4O7RBC",
        "outputId": "3b54e23d-465d-4689-e043-66edc042a914"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to cloud.r-project.org] [Connecting\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to r2u.stat.illinois.edu (192.17.190.167)\r                                                                                                    \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers\r                                                                                                    \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Partie 1** :  Profiling"
      ],
      "metadata": {
        "id": "ZRP_mRecHlmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache4_Optimisation1.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void optimizewithKernel(int *input, int *output, int size) {\n",
        "    extern __shared__ int sharedData[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Charge les √©l√©ments dans la m√©moire partag√©e\n",
        "    if (idx < size) {\n",
        "        sharedData[tid] = input[idx];\n",
        "    } else {\n",
        "        sharedData[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // R√©duction parall√®le\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sharedData[tid] += sharedData[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Le premier thread de chaque bloc stocke le r√©sultat\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 1024;\n",
        "    int *h_input, *h_output, *d_in, *d_out;\n",
        "    h_input = (int*)malloc(size * sizeof(int));\n",
        "    h_output = (int*)malloc(sizeof(int));\n",
        "\n",
        "    // Initialisation des donn√©es\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_input[i] = 1;\n",
        "    }\n",
        "\n",
        "    // Allocation de la m√©moire sur le GPU\n",
        "    cudaMalloc(&d_in, size * sizeof(int));\n",
        "    cudaMalloc(&d_out, sizeof(int));\n",
        "\n",
        "    // Copie des donn√©es de l'h√¥te vers le GPU\n",
        "    cudaMemcpy(d_in, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Lancer le kernel\n",
        "    optimizewithKernel<<<4, 256, 256 * sizeof(int)>>>(d_in, d_out, size);\n",
        "\n",
        "    // Copie du r√©sultat du GPU vers l'h√¥te\n",
        "    cudaMemcpy(h_output, d_out, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Affichage du r√©sultat\n",
        "    printf(\"Sum: %d\\n\", *h_output);\n",
        "\n",
        "    // Lib√©ration de la m√©moire\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OJ6AxN3He7M",
        "outputId": "d547ae81-e4b5-4ac7-9d6a-c509588bde55"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Tache4_Optimisation1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache4_Optimisation1.cu -o Tache4_Optimisation1"
      ],
      "metadata": {
        "id": "2uXg7iJDKGWy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache4_Optimisation1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cYmV503KIz6",
        "outputId": "087c5dc6-3471-4387-b1f9-58213653b675"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==27018== NVPROF is profiling process 27018, command: ./Tache4_Optimisation1\n",
            "Sum: 256\n",
            "==27018== Profiling application: ./Tache4_Optimisation1\n",
            "==27018== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.71%  4.9600us         1  4.9600us  4.9600us  4.9600us  optimizewithKernel(int*, int*, int)\n",
            "                   25.00%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "                   16.29%  1.3760us         1  1.3760us  1.3760us  1.3760us  [CUDA memcpy HtoD]\n",
            "      API calls:   68.05%  129.19ms         2  64.593ms  7.2080us  129.18ms  cudaMalloc\n",
            "                   31.68%  60.134ms         1  60.134ms  60.134ms  60.134ms  cudaLaunchKernel\n",
            "                    0.11%  209.41us         2  104.70us  13.908us  195.50us  cudaFree\n",
            "                    0.11%  203.23us       114  1.7820us     249ns  77.310us  cuDeviceGetAttribute\n",
            "                    0.04%  75.226us         2  37.613us  36.110us  39.116us  cudaMemcpy\n",
            "                    0.01%  15.563us         1  15.563us  15.563us  15.563us  cuDeviceGetName\n",
            "                    0.00%  7.9250us         1  7.9250us  7.9250us  7.9250us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.4430us         1  6.4430us  6.4430us  6.4430us  cuDeviceTotalMem\n",
            "                    0.00%  2.3660us         3     788ns     276ns  1.7670us  cuDeviceGetCount\n",
            "                    0.00%  1.0090us         2     504ns     263ns     746ns  cuDeviceGet\n",
            "                    0.00%     413ns         1     413ns     413ns     413ns  cuModuleGetLoadingMode\n",
            "                    0.00%     337ns         1     337ns     337ns     337ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7T9OkEykULdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "813ChzF9UK5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Partie 2** : Optimisation pour maximiser l'occupation des warps et minimiser les latences"
      ],
      "metadata": {
        "id": "95mUCcs5Kkxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache4_maximisation.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void optimizedKernelLatency(int *input, int *output, int size) {\n",
        "    extern __shared__ int sharedData[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < size) {\n",
        "        sharedData[tid] = input[idx];\n",
        "    } else {\n",
        "        sharedData[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sharedData[tid] += sharedData[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 1024;\n",
        "    int *h_input, *h_output, *d_input, *d_output;\n",
        "\n",
        "    h_input = (int*)malloc(size * sizeof(int));\n",
        "    h_output = (int*)malloc(sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_input[i] = 1;\n",
        "    }\n",
        "\n",
        "    cudaMalloc(&d_input, size * sizeof(int));\n",
        "    cudaMalloc(&d_output, sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 128; // Ajustement de la taille pour maximiser l'occupation\n",
        "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    optimizedKernelLatency<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Sum: %d\\n\", *h_output);\n",
        "\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePFC0oIXK-uK",
        "outputId": "146dc9e2-bf63-4824-c331-671060f3e454"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Tache4_maximisation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache4_maximisation.cu -o Tache4_maximisation"
      ],
      "metadata": {
        "id": "qD50G-cTK-9Z"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache4_maximisation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4QMAxukRRGj",
        "outputId": "07e79847-930c-472f-a81d-6134cb5afc5b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==28516== NVPROF is profiling process 28516, command: ./Tache4_maximisation\n",
            "Sum: 128\n",
            "==28516== Profiling application: ./Tache4_maximisation\n",
            "==28516== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.75%  4.5760us         1  4.5760us  4.5760us  4.5760us  optimizedKernelLatency(int*, int*, int)\n",
            "                   26.19%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "                   17.06%  1.3760us         1  1.3760us  1.3760us  1.3760us  [CUDA memcpy HtoD]\n",
            "      API calls:   71.49%  99.406ms         2  49.703ms  4.5400us  99.401ms  cudaMalloc\n",
            "                   28.21%  39.227ms         1  39.227ms  39.227ms  39.227ms  cudaLaunchKernel\n",
            "                    0.12%  171.51us         2  85.756us  13.904us  157.61us  cudaFree\n",
            "                    0.10%  143.41us       114  1.2570us     134ns  58.026us  cuDeviceGetAttribute\n",
            "                    0.06%  77.027us         2  38.513us  31.715us  45.312us  cudaMemcpy\n",
            "                    0.01%  13.333us         1  13.333us  13.333us  13.333us  cuDeviceGetName\n",
            "                    0.00%  6.6830us         1  6.6830us  6.6830us  6.6830us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.8280us         1  5.8280us  5.8280us  5.8280us  cuDeviceTotalMem\n",
            "                    0.00%  2.2110us         3     737ns     292ns  1.6240us  cuDeviceGetCount\n",
            "                    0.00%     868ns         2     434ns     168ns     700ns  cuDeviceGet\n",
            "                    0.00%     666ns         1     666ns     666ns     666ns  cuModuleGetLoadingMode\n",
            "                    0.00%     224ns         1     224ns     224ns     224ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SqGlXiLsHSgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nous pouvons souligner a partir de cette tache üá∞\n",
        "\n",
        "**Analyse des R√©sultats et Documentation** :  L'utilisation de nvprof a r√©v√©l√© des diff√©rences notables entre la version initiale et la version optimis√©e du kernel. Le temps total d'ex√©cution sur le GPU est pass√© de 4.9600 ¬µs dans la version de base √† 4.5760 ¬µs dans la version optimis√©e, montrant une gestion plus efficace des op√©rations. De plus, le temps pour cudaMalloc a √©t√© r√©duit de 129.19 ms √† 99.406 ms, soulignant une meilleure allocation de la m√©moire. L'ajustement de la taille des blocs et l'optimisation de l'occupation des warps ont contribu√© √† diminuer la latence sans affecter les transferts de donn√©es (cudaMemcpy).\n",
        "\n",
        "***Rapport d'Am√©lioration L'optimisation a permis plusieurs avanc√©es ***: un temps de calcul r√©duit et une meilleure r√©partition des charges ont am√©lior√© l'ex√©cution du kernel. L'alignement des tailles de blocs sur la taille du warp a maximis√© l'occupation et r√©duit les cycles inactifs, augmentant l'efficacit√© globale. L'utilisation de la m√©moire partag√©e et la synchronisation des threads ont renforc√© la scalabilit√©, permettant au programme de g√©rer de plus grandes quantit√©s de donn√©es sans d√©grader les performances.\n"
      ],
      "metadata": {
        "id": "eQXsxsvpTjLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pour conclure* on peut souligner que : *texte en italique*\n",
        "\n",
        "**Pour le Profiling de la 1ere optimisation** : L'utilisation du profiling a permis d'identifier les goulets d'√©tranglement et d'√©valuer les performances pour optimiser le programme.\n",
        "\n",
        "**le profiling a partir d'une Optimisation bas√©e sur les profil**s : Les techniques appliqu√©es ont maximis√© l'occupation des warps et r√©duit la latence, am√©liorant l'efficacit√© et la scalabilit√© pour des traitements intensifs sur GPU."
      ],
      "metadata": {
        "id": "8DK95LXvTkFS"
      }
    }
  ]
}