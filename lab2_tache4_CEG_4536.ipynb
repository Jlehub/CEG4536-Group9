{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratoire 2 de CEG 4536\n"
      ],
      "metadata": {
        "id": "gkbs1NnAR9vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXkJtOEl7M7-",
        "outputId": "48fe41fb-f574-423f-f48e-28beffb76c30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  5 02:29:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5uW4f4O7RBC",
        "outputId": "3b54e23d-465d-4689-e043-66edc042a914"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to cloud.r-project.org] [Connecting\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to r2u.stat.illinois.edu (192.17.190.167)\r                                                                                                    \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers\r                                                                                                    \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                                                    \rHit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Partie 1** :  Profiling"
      ],
      "metadata": {
        "id": "ZRP_mRecHlmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache4_Optimisation1.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void optimizewithKernel(int *input, int *output, int size) {\n",
        "    extern __shared__ int sharedData[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Charge les éléments dans la mémoire partagée\n",
        "    if (idx < size) {\n",
        "        sharedData[tid] = input[idx];\n",
        "    } else {\n",
        "        sharedData[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Réduction parallèle\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sharedData[tid] += sharedData[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Le premier thread de chaque bloc stocke le résultat\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 1024;\n",
        "    int *h_input, *h_output, *d_in, *d_out;\n",
        "    h_input = (int*)malloc(size * sizeof(int));\n",
        "    h_output = (int*)malloc(sizeof(int));\n",
        "\n",
        "    // Initialisation des données\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_input[i] = 1;\n",
        "    }\n",
        "\n",
        "    // Allocation de la mémoire sur le GPU\n",
        "    cudaMalloc(&d_in, size * sizeof(int));\n",
        "    cudaMalloc(&d_out, sizeof(int));\n",
        "\n",
        "    // Copie des données de l'hôte vers le GPU\n",
        "    cudaMemcpy(d_in, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Lancer le kernel\n",
        "    optimizewithKernel<<<4, 256, 256 * sizeof(int)>>>(d_in, d_out, size);\n",
        "\n",
        "    // Copie du résultat du GPU vers l'hôte\n",
        "    cudaMemcpy(h_output, d_out, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Affichage du résultat\n",
        "    printf(\"Sum: %d\\n\", *h_output);\n",
        "\n",
        "    // Libération de la mémoire\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OJ6AxN3He7M",
        "outputId": "d547ae81-e4b5-4ac7-9d6a-c509588bde55"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Tache4_Optimisation1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache4_Optimisation1.cu -o Tache4_Optimisation1"
      ],
      "metadata": {
        "id": "2uXg7iJDKGWy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache4_Optimisation1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cYmV503KIz6",
        "outputId": "087c5dc6-3471-4387-b1f9-58213653b675"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==27018== NVPROF is profiling process 27018, command: ./Tache4_Optimisation1\n",
            "Sum: 256\n",
            "==27018== Profiling application: ./Tache4_Optimisation1\n",
            "==27018== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   58.71%  4.9600us         1  4.9600us  4.9600us  4.9600us  optimizewithKernel(int*, int*, int)\n",
            "                   25.00%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "                   16.29%  1.3760us         1  1.3760us  1.3760us  1.3760us  [CUDA memcpy HtoD]\n",
            "      API calls:   68.05%  129.19ms         2  64.593ms  7.2080us  129.18ms  cudaMalloc\n",
            "                   31.68%  60.134ms         1  60.134ms  60.134ms  60.134ms  cudaLaunchKernel\n",
            "                    0.11%  209.41us         2  104.70us  13.908us  195.50us  cudaFree\n",
            "                    0.11%  203.23us       114  1.7820us     249ns  77.310us  cuDeviceGetAttribute\n",
            "                    0.04%  75.226us         2  37.613us  36.110us  39.116us  cudaMemcpy\n",
            "                    0.01%  15.563us         1  15.563us  15.563us  15.563us  cuDeviceGetName\n",
            "                    0.00%  7.9250us         1  7.9250us  7.9250us  7.9250us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.4430us         1  6.4430us  6.4430us  6.4430us  cuDeviceTotalMem\n",
            "                    0.00%  2.3660us         3     788ns     276ns  1.7670us  cuDeviceGetCount\n",
            "                    0.00%  1.0090us         2     504ns     263ns     746ns  cuDeviceGet\n",
            "                    0.00%     413ns         1     413ns     413ns     413ns  cuModuleGetLoadingMode\n",
            "                    0.00%     337ns         1     337ns     337ns     337ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7T9OkEykULdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "813ChzF9UK5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Partie 2** : Optimisation pour maximiser l'occupation des warps et minimiser les latences"
      ],
      "metadata": {
        "id": "95mUCcs5Kkxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Tache4_maximisation.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void optimizedKernelLatency(int *input, int *output, int size) {\n",
        "    extern __shared__ int sharedData[];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < size) {\n",
        "        sharedData[tid] = input[idx];\n",
        "    } else {\n",
        "        sharedData[tid] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
        "        if (tid < stride) {\n",
        "            sharedData[tid] += sharedData[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int size = 1024;\n",
        "    int *h_input, *h_output, *d_input, *d_output;\n",
        "\n",
        "    h_input = (int*)malloc(size * sizeof(int));\n",
        "    h_output = (int*)malloc(sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        h_input[i] = 1;\n",
        "    }\n",
        "\n",
        "    cudaMalloc(&d_input, size * sizeof(int));\n",
        "    cudaMalloc(&d_output, sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 128; // Ajustement de la taille pour maximiser l'occupation\n",
        "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "    optimizedKernelLatency<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size);\n",
        "\n",
        "    cudaMemcpy(h_output, d_output, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Sum: %d\\n\", *h_output);\n",
        "\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePFC0oIXK-uK",
        "outputId": "146dc9e2-bf63-4824-c331-671060f3e454"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Tache4_maximisation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc Tache4_maximisation.cu -o Tache4_maximisation"
      ],
      "metadata": {
        "id": "qD50G-cTK-9Z"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./Tache4_maximisation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4QMAxukRRGj",
        "outputId": "07e79847-930c-472f-a81d-6134cb5afc5b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==28516== NVPROF is profiling process 28516, command: ./Tache4_maximisation\n",
            "Sum: 128\n",
            "==28516== Profiling application: ./Tache4_maximisation\n",
            "==28516== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.75%  4.5760us         1  4.5760us  4.5760us  4.5760us  optimizedKernelLatency(int*, int*, int)\n",
            "                   26.19%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "                   17.06%  1.3760us         1  1.3760us  1.3760us  1.3760us  [CUDA memcpy HtoD]\n",
            "      API calls:   71.49%  99.406ms         2  49.703ms  4.5400us  99.401ms  cudaMalloc\n",
            "                   28.21%  39.227ms         1  39.227ms  39.227ms  39.227ms  cudaLaunchKernel\n",
            "                    0.12%  171.51us         2  85.756us  13.904us  157.61us  cudaFree\n",
            "                    0.10%  143.41us       114  1.2570us     134ns  58.026us  cuDeviceGetAttribute\n",
            "                    0.06%  77.027us         2  38.513us  31.715us  45.312us  cudaMemcpy\n",
            "                    0.01%  13.333us         1  13.333us  13.333us  13.333us  cuDeviceGetName\n",
            "                    0.00%  6.6830us         1  6.6830us  6.6830us  6.6830us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.8280us         1  5.8280us  5.8280us  5.8280us  cuDeviceTotalMem\n",
            "                    0.00%  2.2110us         3     737ns     292ns  1.6240us  cuDeviceGetCount\n",
            "                    0.00%     868ns         2     434ns     168ns     700ns  cuDeviceGet\n",
            "                    0.00%     666ns         1     666ns     666ns     666ns  cuModuleGetLoadingMode\n",
            "                    0.00%     224ns         1     224ns     224ns     224ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SqGlXiLsHSgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nous pouvons souligner a partir de cette tache 🇰\n",
        "\n",
        "**Analyse des Résultats et Documentation** :  L'utilisation de nvprof a révélé des différences notables entre la version initiale et la version optimisée du kernel. Le temps total d'exécution sur le GPU est passé de 4.9600 µs dans la version de base à 4.5760 µs dans la version optimisée, montrant une gestion plus efficace des opérations. De plus, le temps pour cudaMalloc a été réduit de 129.19 ms à 99.406 ms, soulignant une meilleure allocation de la mémoire. L'ajustement de la taille des blocs et l'optimisation de l'occupation des warps ont contribué à diminuer la latence sans affecter les transferts de données (cudaMemcpy).\n",
        "\n",
        "***Rapport d'Amélioration L'optimisation a permis plusieurs avancées ***: un temps de calcul réduit et une meilleure répartition des charges ont amélioré l'exécution du kernel. L'alignement des tailles de blocs sur la taille du warp a maximisé l'occupation et réduit les cycles inactifs, augmentant l'efficacité globale. L'utilisation de la mémoire partagée et la synchronisation des threads ont renforcé la scalabilité, permettant au programme de gérer de plus grandes quantités de données sans dégrader les performances.\n"
      ],
      "metadata": {
        "id": "eQXsxsvpTjLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pour conclure* on peut souligner que : *texte en italique*\n",
        "\n",
        "**Pour le Profiling de la 1ere optimisation** : L'utilisation du profiling a permis d'identifier les goulets d'étranglement et d'évaluer les performances pour optimiser le programme.\n",
        "\n",
        "**le profiling a partir d'une Optimisation basée sur les profil**s : Les techniques appliquées ont maximisé l'occupation des warps et réduit la latence, améliorant l'efficacité et la scalabilité pour des traitements intensifs sur GPU."
      ],
      "metadata": {
        "id": "8DK95LXvTkFS"
      }
    }
  ]
}